\chapter{Design and implementation of the event detection and summarisation methodology}\label{DesignAndImplementation}
\ifpdf
    \graphicspath{{Chapter3/Chapter3Figs/PNG/}{Chapter3/Chapter3Figs/PDF/}{Chapter3/Chapter3Figs/}}
\else
    \graphicspath{{Chapter3/Chapter3Figs/EPS/}{Chapter3/Chapter3Figs/}}
\fi

The aim of this chapter is to give a thorough description of the implementation of our methodology for event detection and summarisation. The theoretical concepts introduced in Chapter \ref{TheoreticalFramework} are now the building blocks of this methodology. Each of these building blocks can be used in isolation but when we combined these individual components the end product is a data mining toolset which is able to detect and summarise events. We list and provide an explanation of the individual components of this toolset and also explain our reasoning for certain design choices. 

\section{System Overview}
Figure \ref{SystemOverview} shows an overview of the system architecture which is a pipeline of the individual components we described
in Chapter \ref{TheoreticalFramework}. Each one of these components is depicted as an independent module in the figure.  
Initially, historical tweets from a service provider (Twitter API or another provider) are retrieved and stored in an 
appropriate format in the database. Subsequently, the system receives a stream of tweets from the database and processes and 
transforms them in a format that is appropriate for clustering. The next step in the pipeline is the actual clustering of the tweets 
in order to detect groups of tweets discussing the same topic. Then, the extracted clusters are processed in order to identify the events 
and generate their summaries. Finally, a visual representation of the results should be generated in order to aid understanding of the events.\\

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=3in, width=6in]{system-overview}
    \caption{System overview - The event extraction system comprises of several independent components.}
    \label{SystemOverview}
  \end{center}
\end{figure}

\subsection{Tools}
In the process of implementing the system we have mainly used our implementation of the algorithms but several third-party 
software libraries were used to implement the subcomponents of the system. Here we describe the main tools we have used.\\ 

\begin{itemize}
 \item \textbf{Python\footnote{http://docs.python.org/tutorial/}:} Python is a powerful programming language with efficient high-level data structures. We have selected Python due to its wide range of third-party libraries which can be used for mathematical operations. Additionally, is an ideal language for scripting and rapid application development which is desired in our project since we wanted to test our implementation as quickly as possible.  
 \item \textbf{Natural Language Toolkit\footnote{http://nltk.org/} (NLTK):} NLTK is a complete platform for building Python programs to work with human generated text content. It provides built-in methods for text tokenisation and sentence segmentation as well as modules for named-entity recognition.    
 \item \textbf{Lucene\footnote{http://lucene.apache.org/core/}:} Lucene is a high-performance, full-featured text search engine library written entirely in Java. Lucene allows us to build search engines or any other application that requires full-text search. In our case we use Lucene to build our inverted index structure. More specifically, we use PyLucene which is a library with Python wrappers for Lucene's functions.
 \item \textbf{Orange\footnote{http://orange.biolab.si/}:} Orange is an open-source data mining library. It contains a wide range of methods to perform several data mining and machine learning methods such as classification and clustering.    
 \item \textbf{Numpy\footnote{http://numpy.scipy.org/}:} NumPy is a Python library which is concerned with adding support for large, multi-dimensional arrays and matrices. It is used throughout our implementation to perform all the matrix and vector operations.
\end{itemize}\vspace{15pt}

\section{Data Retrieval}
A vital part of our system is the retrieval of a large amount of historical tweets. The first obvious choice is the Twitter API which provides tweets, user profiles and several metadata related to Twitter. They also provide a streaming API which is commonly used to collect tweets in real time. However, the main problem with the Twitter API is that it has a very restrictive limit policy (150 requests per hour) and it does not provide access to tweets posted more than a few days ago. This raises a significant barrier for our project since we require access to historical data. The solution is to use other archiving services and there are numerous possibilities. Additionally, it is essential for us to have direct access to their database through an API and unfortunately, most of them do not provide an API. We have found that Topsy \footnote{http://topsy.com/} provides an excellent API \footnote{http://code.google.com/p/otterapi/} and direct access to tweets covering a period from 2009 up to the present day. Additionally, Topsy API is free and the limit policy allows us to retrieve our data easily. Therefore, we have decided to use Topsy Otter API with its Python bindings.

\section{Raw text processing}
The raw tweets received from Topsy are not processed yet and therefore we must apply some preprocessing steps before storing them in the database. The reasons for pre-processing were outlined clearly in section \ref{RawDataProcessing}. Figure \ref{RawTextProcessingOverview} depicts the subcomponents of the raw text preprocessing module.\\\\

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=1.5in, width=6in]{raw-data-processing}
    \caption{The raw data processing module - All the steps neccessary to convert raw documents to a format suitable for storage in a database.}
    \label{RawTextProcessingOverview}
  \end{center}
\end{figure} 
\noindent \textbf{HTML and URL removal:} Firstly, we need to remove the URLs and HTML tags from the tweets since they are useless for clustering. In order to do so we have used regular expressions which capture any possible format of URLs or HTML code.\\\\
\textbf{Sentence segmentation and tokenisation:} For the implementation of this module we have used the default sentence segmenter of NLTK and the WordPunctTokenizer to tokenise the resulting sentences. The reason behind the choice of the WordPunctTokenizer is due to the fact that it can handle alphabetic and non-alphabetic characters. Since it is common to use non-alphabetic characters in a tweet, this tokeniser made it easy to remove characters such as '.' and ','. Consider for example the following tweet:
 
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=1in, width=4in]{tweet-text}
    \label{TweetText}
  \end{center}
\end{figure}

\noindent The output from this module will be a list of words containing the terms \textbf{[ 'as', 'egyptians', 'prepare', 'for' 'jan25', 'protests', 'mumbarak', 'has', 'turned', 'egypt', 'into', 'a', 'police', 'state', 'where', 'torture', 'police', 'brutality', 'r', 'systematic' ]}. Note that characters '.', '\#', '\&' and ':' have been removed. \\

\noindent \textbf{Stopword removal:} The next step is to remove common English words that do not provide any information. NLTK provides a dictionary of the English stopwords and we have used it to filter out stopwords from the tweets. Using the list of words extracted for the example above the output of the stopword removal module will be: \textbf{['egyptians', 'prepare', 'jan25', 'protests', 'mumbarak', 'turned', 'egypt', 'police', 'state', 'torture', 'police', 'brutality', 'r', 'systematic' ]}

\noindent \textbf{Stemming:} Once we have the list of our terms we can use a stemming algorithm to reduce the words to their root. Our implementation uses the widely used Porter stemmer which is also implemented in NLTK. The final list of words after the stemming becomes \textbf{['egyptian', 'prepar', 'jan25', 'protest', 'mumbarak', 'turn', 'egypt', 'polic', 'state', 'tortur', 'polic', 'brutal', 'r', 'systemat' ]}\\

\noindent \textbf{Indexing:} Just before storing the tweets in the database, we take a last step which is to index the tweets. For each word occurring in our corpus we aggregate all the tweets, that contain that term, and the position of that word in the document. Effectively, we create a mapping from a word to a list of documents. In our implementation the inverted index is very important since it allows us to construct term-frequency vectors easily and filter terms and documents. For example, using our index we can find the words that appear either too often or less frequently and filter them out. This is used to reduce the dimensionality of our dataset by removing unnecesary words. Alternatively, we
can remove documents/tweets which contain keywords that appear too often or less frequently. In our implementation we have used PyLucene which is the Python equivalent of the Lucene indexing library. The library allow us to do what we have discussed above and in addition it provides helper functions such as the calculation of the TF-IDF weightings for a dataset.
 
\section{Clustering}
So far we have managed to retrieve and store historical tweets in our database. The next module in our pipeline is to perform clustering on the tweets and identify clusters which discuss the same topic. The clustering module (Figure \ref{ClusteringOverview}) must construct the term-frequency vectors by processing the dataset and subsequently to apply a clustering algorithm on the dataset. 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=1.5in, width=4in]{clustering-overview}
    \caption{The cluster analysis module - The dataset is processed to construct the term-frequency vectors and then a clustering algorithm is used to identify the clusters.}
    \label{ClusteringOverview}
  \end{center}
\end{figure} 

\noindent There are several candidates algorithms for clustering documents and we have decided to implement four of them. The theoretical background of these algorithms is outlined in Chapter \ref{TheoreticalFramework} and in Chapter \ref{Evaluation} we present a thorough comparison of these algorithms with respect to their performance in clustering tweets.  
In the next section we provide our implementation of these algorithms and the components that are needed for clustering. All the clustering and summarisation algorithms were implemented by us using the help of the aforementioned third-party tools.  

\subsection{Software implementation}
The four different algorithms presented in this section are the k-Means, DBSCAN, Non-negative Matrix Factorisation (NMF) and online clusterers. Although these methods are fundamentally different they share some common functionality. For example the preprocessing steps, such as the construction of the term-frequency vectors is identical for all of them. Also, all of the algorithms output the clusters using the same format and therefore the methods for visualising the clusters will be identical. The architecture of the software components responsible for implementing the clustering functionality tries to incorporate this common functionality as well as accomodating the different core implementation of each algorithm. The UML diagram (Figure \ref{UMLClusterers}) below illustrates the different components of the clustering module. The AbstractClusterer class implements all the common functionalities such as add\_document, construct\_term\_freq\_matrix, plot\_scatter and dump\_clusters\_to\_file. Since each algorithm uses a different method to perform the actual clustering task, the AbstractClusterer class does not provide any implementation for the "run" function. This is the responsibility of each derived clusterer. The use of an AbstractClusterer makes our design extremely modular and reusable since we can switch different implementations at any time with minimal changes in the code.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=2.0in, width=6in]{clusterers-uml}
    \caption{The four derived clusterers share common functionality which is implemented in the AbstractClusterer class. }
    \label{UMLClusterers}
  \end{center}
\end{figure}    

\subsubsection{AbstractClusterer}
\noindent The main functionality of the AbstractClusterer is to construct the term-frequency vector for each document and consequently the term-frequency matrix $a$ which has a row for each document and a column for each distinct term in the dataset. This will convert the corpus in the vector space representation. Listing \ref{AbstractClustererSnippet} shows the pseudocode for implementing the construct\_term\_freq\_matrix() function. It retrieves the dataset from the database and finds all the distinct terms that occur across all the documents. Then the term-frequency matrix is initialised and we iterate over the documents filling in the elements of the matrix with the TF-IDF weight of each term. The function returns the matrix $a$.

\begin{lstlisting}[language=Python, label=AbstractClustererSnippet, caption=Pseudocode for constructing the term-frequency matrix for a dataset]
def construct_term_freq_matrix(corpus):
  '''
  Inputs: 
  corpus: The collections of documents we will cluster
  
  Outputs:
  a: A nxm matrix where n is the number of documents 
     in the corpus and m the number of distinct terms
     in the corpus.  
  '''
  terms = find_distinct_terms(corpus)
  a = initialise_term_frequency_matrix(rows=len(corpus), 
                                        columns=len(terms))
  
  for i, document in enumerate(corpus):
    for term in document
      a[i][term] = tf_idf(term, document, corpus)
    
  return a 
\end{lstlisting}

\noindent In the next pages we discuss the implementation of the concrete clusterers which implement the second subcomponent of the cluster analysis module.

\subsubsection{k-Means clusterer implementation}
Listing \ref{KmeansClustererSnippet} shows the pseudocode for k-Means clustering algorithm. The algorithm works by randomly initialising k centroids and then finding out which document vectors are closest to the centroid. Then the centroid is recomputed based on the mean of the vectors belonging to this centroid and this mean value becomes the new centroid. An additional check is performed again to ensure that all the vectors are still belonging to that cluster after the recomputation of the centroid. This procedure is repeated until there are previous cluster membership has not changed. Note that the distance() function can be any of the three distance measures described in the previous chapter.

\begin{lstlisting}[language=Python, label=KmeansClustererSnippet, caption=Pseudocode for k-Means algorithm]
def run(k, document_vectors):
  '''
  Inputs: 
  k: the number of clusters
  document_vectots: the set of n document 
                     vectors I={i1,i2,..,in}
  
  Outputs:
  C: cluster centroids {c1, c2, ..., ck}
  m: I --> C the cluster membership
  '''
  C = random_centroid_initialisation()
  
  distances = []
  for vector in document_vectors
    for c in C:
      dist = distance(c, vector)
      distances.append(dist)
    m[vector] = min(distances)

  while has_changed(m):
    for c in C:
      c = recompute_centroid(c, vectors_of(c))   
    
    for vector in document_vectors
      for c in C:
        dist = distance(c, vector)  
        distances.append(dist)
      m[vector] = min(distances)
  return C, m

\end{lstlisting}
\subsubsection{DBSCAN clusterer implementation}
Listing \ref{DbscanClustererSnippet} shows the pseudocode for the DBSCAN clustering algorithm. Initially,
all documents are marked as unvisited and the cluster list is empty. Then the algorithm randomly selects a new document vector, marks it as visited and finds all its neighbours than are within a distance $eps$. The set of neighbours is called $\epsilon$-neighbourhood and we denote it here by N. In order to calculate the distance we use one of the three predefined distance measures. If N does not contain at least min\_pts then we mark this vector as a noise point. Otherwise, a new cluster is created which contains this vector and all the objects in N are added to a candidate set. DBSCAN iteratively adds to the new cluster those documents in the N that do not belong to any cluster. For any object in the $\epsilon$-neighbourhood DBSCAN checks again its own $\epsilon$-neighbourhood N' and if it has at least min\_pts those document vectors are added to the N. This continues, with DBSCAN adding new documents in the new cluster, until N is empty. Finally, to find the next cluster DBSCAN selects a new unvisited document vector and continues the same process until all vectors have been visited.  

\begin{lstlisting}[language=Python, label=DbscanClustererSnippet, caption=Pseudocode for DBSCAN algorithm]
def run(min_points, eps, document_vectors):
  '''
  Inputs: 
  eps: the radius parameter
  min_pts: the neighbourhood density threshold.
  document_vectors: the set of n document vectors 
                     I={i1,i2,..,in}
  
  Outputs:
  C: the clusters and the document vectors 
     belonging to them
  '''
  C = [] #The cluster list
  mark_all_vectors_as_unvisited(document_vectors)
  
  while not all_is_visited(document_vectors):  
    vector = pick_random(document_vectors)
    mark_as_visited(vector)
    neighbours = get_neighbours(document_vectors, vector)
    if len(neighbours) >= eps:
      new_cluster = create_cluster(vector) 
      C.append(new_cluster)
      for n_vector in neighbours:
        if not is_visited(n_vector) 
          mark_as_visited(n_vector)
          n_neighbours=get_neighbours(document_vectors,n_vector)
          if len(n_neighbors) >= eps:
            neighbours.append(n_neighbours)
        if not_in_cluster(n_vector):
          new_cluster.add(n_vector)
    else:
      mark_as_noise(vector) 
  return C
\end{lstlisting}
\subsubsection{Non-negative matrix factorisation (NMF) clusterer implementation} Our NMF algorithm implementation is based on the paper [put ref here for Learning the parts of objects by nonnegative matrix factorization]. The method accepts as input the term-frequency matrix $V$ the number of basis vectors to generate $r$ and the number of optimisation iterations. The algorithm starts from non-negative initialisations for W and H and then iteratively updates W and H until a factorisation $V \approx WH$ such that $| V - WH |$ is minimal. If the number of maximum iterations has not been reached and the error did not change since the last iteration then the algorithm halts and return W and H. Listing \ref{NmfClustererSnippet} shows the pseudocode for the NMF algorithm. The notation $A.T$ indicates that we should take the transpose of the matrix $A$.
  
\begin{lstlisting}[language=Python, label=NmfClustererSnippet, caption=Pseudocode for the NMF algorithm]
def run(V, r, iterations):
  '''
  Inputs: 
  V: the matrix to factorize
  r: number of basis vectors to generate
  iterations: number of optimisation
             iterations to perform
  
  Outputs:
  W: a set of r basis vectors
  H: represenations of the columns of V in 
     the basis given by W
  '''
  
  C = size(V,1) #dimensionality of examples (# rows)
  N = size(V,2) #number of examples (columns)

  W = rand(N,r);
  H = rand(r,C);
  
  previous_error = 0
  error = 0
  for i in xrange(iterations):

    #Update W
    W2 = dot(dot(W, H), H.T) + 10**-9
    W *= dot(V[:,:], H.T)
    W /= W2
    
    #Update H
    H2 = dot(dot(W.T, W), H) + 10**-9
    H *= dot(W.T, V[:,:])
    H /= H2                                     
    
    previous_error = error
    error = sqrt( sum((V[:,:] - dot(W, H))**2 ))
    
    if i > 1 and error:
      if previous_error == error:
        break
  return W, H
  
\end{lstlisting}

\subsubsection{Online clusterer}
The previous algorithms operated on the dataset as a whole. Since we are interested in clustering a vast amount of documents these approaches are expensive. However, we can use a category of clustering algorithms called sequential or online clustering methods. The main difference is that they cluster each individual example sequentially by updating the clusters once this specific example is presented to the system. Therefore, an online algorithm is scalable. Usually, these kinds of algorithms are centroid-based and the mean values of centroids are updated using a moving average. This fact makes an online clusterer much faster than the other algorithms.\\\\
In our case, whenever we are presented with a new document the term-frequency vectors of all the previous documents must be updated. This operation is computationally expensive and we have implemented a simple refinement of this algorithm. Our version of the algorithm allows only a sliding window of $N$ tweets to be in memory each time the term-frequency vectors are calculated. For example if we are presented with the $ith$ document, denoted as $x_i$ and our window size is N then only the documents $\{ x_{i-1}, x_{i-2},..., x_{i-N}\}$ will be used for clustering. This can provide us with an extra performance boost but at the same time it decreases accuracy since we calculate our feature vectors on a limited number of documents.\\\\
Our implementation is illustrated in pseudocode in Listing \ref{OnlineClustererSnippet} and it can be summarised in three main steps: 

\begin{enumerate}
  \item Find the closest centroid and move it closer to the new document vector. 
  \item Merge the two closest centroids, c1 and c2, which means that we are left with a redundant centroid c2.
  \item Set the redundant centroid equal to the document vector. 
\end{enumerate}
These three steps satisfy three important criteria. Firstly, the within-cluster variance is minimised by Step 1 and the between-cluster distance is maximised by Step 2. Finally we capture the changes in the data distribution after a new document is clustered by Step 3 since we are treating each new document as an indication to a potential new cluster.\\\\
In the algorithm there are two cases when we need to update the location and the size of an existing centroid. The first one is when a centroid is the closest to a new document and it must be moved closer to it. The location of the closest centroid, $c$, is updated according to the following update rule location:
\begin{eqnarray}
c_{center} \leftarrow c_{center} + \frac{d_{center} - c_{center}}{c_{size} + 1}    
\end{eqnarray}  
where $c_{center}$ is the centroid's location, $c_{size}$ is the number of documents this centroid represents and $d_{center}$ is the document that we are trying to get closer to. The size of a centroid is increased by one each time a new document is assigned to it. The second case for updating is when two clusters, $c_1$ and $c_2$ need to be merged. The update rules for the location and the size are then defined as:
\begin{eqnarray}
c1_{center} \leftarrow \frac{c1_{center} \times c1_{size} + c2_{center} \times c2_{size}}{c1_{size} + c2_{size}} \\
\end{eqnarray}  
\begin{eqnarray}
c1_{size} \leftarrow c1_{size} + c2_{size} 
\end{eqnarray}  


\begin{lstlisting}[language=Python, label=OnlineClustererSnippet, caption=Pseudocode for the online clustering algorithm]
def run(clusters, k_max, N, document):
  '''
  Inputs:
  N: the size of the document window
  k_max: the maximum number of clusters we can identify 
  document: the new document to be clustered 

  Outputs:
  clusters: returns the updated clusters
  '''    
  #construct a new term-frequency vector based on the new
  #document and consider only the last N documents. 
  vector = construct_term_freq_vector(self.document_list, 
                                      document, 
                                      N)

  distances = []
  for centroid in clusters:
    dist = distance(centroid, vector)
    distances.append(dist)
  
  #find the closest centroid (Step 1)
  closest = min(distances)
  closest.add(vector)

  #update the centroids center  
  closest.size += 1
  closest.center += (vector-closest.center)/closest.size
  
  if len(clusters)>=k_max and len(clusters)>1:
    #merge the most similar clusters (Step 2)
    merged = merge_closest(clusters)
  
  
  #create a new cluster for this document (Step 3)
  newc = Cluster(vector)
  clusters.append(newc)
  return clusters
  
def merge_closest(clusters):
  c1, c2 = find_closest()
  c1.center=(c1.center*c1.size+
            c2.center*c2.size)/(c1.size+c2.size)
  c1.size+=c2.size
  c2.remove() #c2 is now merged with c1 so remove it
  return c1
    
\end{lstlisting}

\section{Identifying events}\label{IdentifyEvents}
[TODO: Complete this section]

\section{Generating automatic summaries}
So far we have managed to retrieve historical tweets from Twitter and identify clusters of topics that were discussed in them. After that we have filtered out the irrelevant clusters and kept the important ones which are considered as the events. Some of these events contain a lot of tweets and therefore it would be extremely tedious for a human observer to understand what happened during that event. Therefore, at this point we must implement our automatic summary generators. The main task here is to find the most important tweets, keywords and named entities. In the next sections we describe how we have implemented the functionality to extract this information. An overview of the summarisation module is shown in Figure \ref{SummarisationOverview}.  

\begin{figure}[!htbp]
  \begin{center}
    \includegraphics[height=3.0in, width=6in]{summarisation-overview}
    \caption{The summarisation module receives the event clusters and outputs the highest ranked tweetsm keywords and entities in each cluster.}
    \label{SummarisationOverview}
  \end{center}
\end{figure} 

\subsection{Ranking tweets}
The first subcomponent of the summarisation module is to device a way to rank the tweets in order to extract the most \emph{relevant}, \emph{useful} and \emph{high-quality} tweets. These are the three requirements that our summarisers must satisfy. We described them in more detail in Chapter \ref{TheoreticalFramework} and in the following sections we will take a look at the implementations of centroid-based summarisation method and LexRank.

\subsubsection{Abstract summariser}
Similarly to the clustering module, the two summarisers share common functionality and we have defined an AbstractSummariser which is responsible to implement these common functions. The derived summarisers will inherit these functions and also provide their own concrete implementation of the "run" (the function which performs the actual ranking) function. Figure \ref{SummariserArchitecture} illustrates the software architecture. 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=2.0in, width=4in]{summarisers}
    \caption{The two derived summarisers share common functionality which is implemented in the AbstractSummariser class. }
    \label{SummariserArchitecture}
  \end{center}
\end{figure} 

\subsubsection{Centroid-based summariser}
The main idea behind this summariser is that tweets closer to the event's cluster centroid are more likely to be \emph{useful} and \emph{relevant} to the event than others which are distant. Also, the third criterion (quality) is met since the centroid's weights are calculated based on the average of the tweets' vectors and therefore typos and spelling mistakes are more likely to be ruled out. The implementation of this algorithm is straightforward and is shown in Listing \ref{CentroidSummariserSnippet}. First we retrieve the term-frequency vectors for all the documents in the cluster we want to rank. Then we calculate the centroid by averaging the individual vectors. Finally, the cosine similarity of each document and the centroid is calculated and we return a sorted list of similarities. The similarity acts as a ranking score since the closer a document is to a centroid the highest its ranking.  

\begin{lstlisting}[language=Python, label=CentroidSummariserSnippet, caption=Pseudocode for the centoid-based summariser.]
def run(event_cluster):
  '''
  Inputs:
  event_clusters: The cluster for which we want to rank 
                  the tweets.
  Outputs:
  ranked: A list of ranked tweets in descending order. 
  '''        
  vectors = event_cluster.get_document_vectors()
  
  #Calculates the centroid by averaging the individual
  #vectors of alll the documents in the cluster.
  centroid = calculate_centroid(vectors)
  
  similarities = []
  for vector in vectors:
    sim = cosine(vector, centroid)
    similarities.append(sim)
  
  return sorted(similarities)
  
\end{lstlisting}

\subsubsection{LexRank summariser}
As we have described in Chapter \ref{TheoreticalFramework} 
LexRank is an algorithm where we define a graph which consists of nodes representing the sentences in the text to be summarised and the 
edges are placed between two sentences that are similar to each other. We can then rank all the sentences 
based on the expected probability of a random walker visiting each sentence using equation \ref{LexRankEquation}. \\
The main difference between our implementation and the original LexRank algorithm is that we scale the probability of 
jumping to the next node based on the cosine similarity of two nodes. The reason for this is that we want to favor jumping to 
a node that discusses a more similar topic. We can implement this change by scaling the summation in the equation by the cosine similarity 
of the nodes $u$ and $v$. Since the similarity is in the range $[0, 1]$ an identical node it won't be discounted whereas a node with similarity equals 0 
it will be ignored. The equation the becomes:

\begin{eqnarray}\label{LexRankEquationModified}
L(u) = \frac{d}{N} + (1-d) * sim(u, v) \sum_{v \in adj[u]}^{\infty}\frac{L(v)}{deg(v)}
\end{eqnarray} 

\begin{lstlisting}[language=Python, label=CentroidSummariserSnippet, caption=Pseudocode for the centoid-based summariser.]
def run(event_cluster, threshold, tolerance):
  '''
  Inputs:
  event_clusters: The cluster for which we want to rank 
                  the tweets.
  Outputs:
  ranked: A list of ranked tweets in descending order. 
  '''        

  vectors = event_cluster.get_document_vectors()      
  n = len(vectors)
  adjacency_matrix,degree=calculate_similarities(vectors, 
                                              threshold)
  
  for i in xrange(n):
      for j in xrange(n):
        adjacency_matrix[i][j] = adjacency_matrix[i][j] 
                                            / degree[i]
                                            
  ranked = power_method(adjacency_matrix, tolerance)        
  
  return ranked
    
def power_method(self, m, epsilon):
  n = len( m )
  p = [1.0 / n] * n
  while True:
      new_p = [0] * n
      for i in xrange( n ):
          for j in xrange( n ):
              new_p[i] += m[j][i] * p[j]
      total = 0
      for x in xrange( n ):
          total += ( new_p[i] - p[i] ) ** 2
      p = new_p
      if total < epsilon:
          break
  return p  
\end{lstlisting}

\subsection{Named entity and keyword extraction}
The other two parts of the summarisation module is the named entity and keyword extraction. The keyword extraction task is straightforward since we merely
find the keywords in the event cluster that have the heighest TF-IDF weights. For the name entity extraction task we decided to use the built-in capabilities of NLTK. More specifically, NLTK contains functions which can implement part of speech tagging and then extract named entities based on these tags\footnote{The pseudocode for this task is not included since it has not been implemented by us.}. For example if the input to the system is a tweet with the following content: "Most protesters in Cairo have gathered in front of the Maspiro building, protest in Alex is also picking up â€ª\#jan25" then the output will be "Cairo, GPE" meaning that Cairo is identified as a location. Named entity extraction is not always accurate and therefore we must be careful when we decide which entities to output. For example in the above tweet the system could erroneously identify the word "Alex" as a person's name. However, in reality the author of the tweet mentioned the city Alexandria in Egypt. Therefore, in order to minimise the possibility of a misinterpretation we decided to output only named entities that appear very frequently in a cluster. 

\section{Classifying users}
In our system user classification is an important feature since it can allows us to understand the events even better. In this section we describe the implementation of automatic classification of users. The motivation behind this is the fact that we would like to be able to filter tweets according to their authors. For example, we might want to extract events based only on tweets from journalists. Since we want to investigate the events that took place during the Arab Spring we know that the important types of users we need to consider are: Media organisations, Journalists, Activists, Celebrities and Common individuals. In order, to classify a user to one of these categories we must construct its profile. A user profile is an N-dimensional vector of attributes that are used to discriminate different user types. We have narrowed down these attributes to be:
\begin{itemize}
  \item Retweet ratio
  \item Tweets conatining links ratio
  \item How often does this user get retweeted?
  \item Ratio of replies
  \item Ratio of mentions
  \item Followers to followees ratio 
\end{itemize}
The first attribute is the number of retweets of a user divided by the overall number of their tweets. Retweets are mainly used by ordinary individuals to share a tweet they liked but they are rarely used by media organisations. Therefore, if the retweet ratio is very small we may suspect that this is a media organisation. The second attribute denotes the number of tweets containing links divided by the total number of tweets of a user. This is also an important feature because we can very easily identify a media organisation from the number of tweets that contain links. The reason is because Twitter accounts for media organisation are mainly used to share links of their articles. If a user gets retweeted too often this is usually an indication that they are popular or their tweets are considered interesting. Therefore, the number of times a user gets retweeted can be a measure of its popularity and therefore if we observe a user which gets retweeted frequently we may assume they are a celebrity, a journalist or even an activist. A user can reply to another user by using the "@" character followed by the username and the same holds for when a user wishes to mention another user in a tweet. The difference between a mention and a reply is that replies have the "@username" pattern in the beginning, whereas in mentions this pattern can appear anywhere. Replies are usually used by ordinary individuals to start and maintain conversations but are very rarely used by celebrities and media organisations. Finally, one of the most discriminating features is the followers to followees ratio. The reason for this is because celebrities and media organisations have very high followers to followees ratio while ordinary individuals have very low ratio. Activists and journalists are in the middle of the spectrum.\\\\
The task now becomes to collect a training dataset which contains labelled user profiles. This will be used to train our classifiers. The obvious way to collect the data is by manually looking up users on Twitter and constructing their profiles. However, this will be a tedious and time consuming process and thus we decided to collect our data automatically. We have achieved this by crawling a website called Twtrland \footnote{http://twtrland.com/} which provides Twitter user statistics.   

\subsection{Crawling user profiles}
A web crawler is a type of software agent that browses websites in a methodical and automated way in order to retrieve up-to-date information. This technique is used in our case to retrieve user profiles from Twtrland. Our web crawlers are constructed using an open source crawling library for Python called Scrapy \footnote{http://scrapy.org/}. A crawler is given the URL of a user profile on Twtrland and then it parses the HTML code of that page retrieving the information we need. A typical page on Twtrland is shown in Figure \ref{TwtrlandPage} and the red rectangles indicate the information we collect. 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=1.9in, width=6in]{twtrland-page}
    \caption{A typical Twtrland user profile. The information in the circles is retrieved by our crawler and used in the construction of the user feature vector.}
    \label{TwtrlandPage}
  \end{center}
\end{figure} 

\subsection{Constructing the feature vectors}
Based on the information we have collected using our crawlers we can very easily construct a feature vector for each user. The vector has one column for each of the six attributes we 
defined above. Some examples of feature vectors are shown in Table \ref{FeatureVectors}. The columns represent the username, retweet ratio, link ratio, how often this user gets retweeted, replies ratio, mentions ratio and followers to followees ratio (FF) respectively. 

\begin{table}[htbp]
\footnotesize
\centering
\begin{tabular}{ l  l  l  l  l  l  l }
  \hline
  \textbf{User} & \textbf{Retweets} & \textbf{Links} & \textbf{Retweeted} & \textbf{Replies} & \textbf{Mentions} & \textbf{F/F} \\ \hline
  \emph{nytimes} & 0.0540 & 0.9091 & 0.6700 & 0.0043 & 0.0033 & 7089.2700 \\
  \emph{aplusk} & 0.0970 & 0.4042 & 0.5000 & 0.1971 & 0.0880 & 14151.1944 \\
  \emph{bencnn} & 0.2702 & 0.0974 & 0.4500 & 0.0988 & 0.0093 & 186.1068 \\
  \emph{alaa} & 0.2500 & 0.0407 & 0.8700 & 0.5120 & 0.0733 & 134.4679 \\
  \hline
\end{tabular}
\caption{The feature vectors of different types of users users}
\label{FeatureVectors}
\end{table}

\noindent The first user is the Twitter account of New York Times and it is representative of the feature vectors for media organisations. Low retweet rate and a large proportion of their tweets contain links as expected. Their Followers to Followees (FF) ratio is very high and one can easily infer by their low reply and mention ratii that they almost never engage in a conversation. The second example is Ashton Kutcher, a famous Hollywood actor. His FF ratio is very high, as expected, and another important observation is that his replies ratio is relatively high meaning that he usually chats with other users. The username bencnn belongs Ben Wedeman a famous American journalist working for CNN. An interesting observation is that his tweets are retweeted a lot. More specifically, he gets retweeted every 0.45 tweets which is the highest rate from all four examples. This is expected as he usually tweets content that people find interesting and worth sharing. Finally, the fourth example profile belongs to Alaa Abd El-Fattah, a well known Egyptian political activist. In his profile we can observe relatively low FF ratio but very high replies ratio which is expected as he usually engages in conversations with other users.

\subsection{Software architecture}
Figure \ref{Classifiers} depicts the software architecture of the user classification module. Once again we wanted to make it as easy as possible to swap implementations of classifiers and therefore
the AbstractClassifier class factors out the common functionality of classifiers leaving the concrete implementations to the derived classes. 

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=2in, width=4in]{classifiers}
    \caption{The two derived classifiers share common functionality which is implemented in the AbstractClassifier class. }
    \label{Classifiers}
  \end{center}
\end{figure} 

\subsubsection{ID3 Classifier implementation}
One of the most popular decision tree learning algorithms is the ID3 and there are a lot of open source implementations. It learns a decision tree by constructing it top-down iteratively and 
at each iteration it tries to find the best attribute from the feature vector to be tested, i.e. the attribute that helps the most in discriminating the examples. It selects this attribute using
a statistical test to determine how well it classifies the training examples.\\\\
We consider decision trees to be an excellent choice in our case since they are less prone to noise in the data than other algorithms. Our dataset is likely to contain noisy user profiles since some of the user profiles are misleading due to the existence of automatic tweet bots and fake accounts. The effect of this noisy examples can be ruled out by the decision tree learning algorithm.  
We have decided to use Orange's ID3 implementation which can be easily extended to incorporate new functionality. In particular, in our implementation we have written wrapper functions for training and testing a classifier. We train the classifier with the labelled user examples we have collected using our crawlers and then using the learned tree we can classify unknown examples. A part of the learned tree based on our training dataset is shown in Figure \ref{Tree}. Leaf nodes in this tree are labelled with numbers in the range $[0, 4]$ where each number indicates a different type of user (0: Celebrity, 1: Media Organisation, 2: Journalist, 3: Activist, 4: Ordinary Individuals). ID3 selected the root node to be the FF ratio and this is expected since this attribute is usually enough to get an indication of the type of the user. If the FF ratio for an example is lower than 143.068 then we move to the left branch otherwise to the right one. We keep moving downwards the tree until we hit a leaf node which gives the final classification for this example. For instance if an example has FF ratio higher than 143.068, it gets retwetted in less than 0.070 tweets and the proportion of links in the tweets is lower than 0.043 then the we hit the leaf node labelled with the number $0$. This number indicates that the class of this example is "Celebrity".

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=3in, width=6in]{tree}
    \caption{Part of the decision tree learned by our classifier.}
    \label{Tree}
  \end{center}
\end{figure} 

\subsubsection{k-Nearest Neighbours Classifier implementation}
Another approach for classifying users is the k-Nearest neighbour which is again effective in handling noisy datasets given a relatively large training dataset. Once again we have used the k-Nearest neighbour algorithm implemented with Orange and in Chapter \ref{Evaluation} we compare its performance against the decision tree implementation. The algorithm receives the training data, in the same way as the ID3 algorithm does, and then we can present it with a new example to get its classification. As it is expected a lazy learning algorithm like k-Nearest neighbour takes longer to classify an example but much less time during training since effectively there is no training phase.   

\section{Summary}
This chapter presented the implementation of different subcomponents of a data mining toolset. The chapter's section proceeded step-by-step through all the stages of our methodology, from data retrieval and text preprocessing to clustering and summarisation. When the various components of this toolset are connected, they should construct an event detection and summarisation system. In the next chapter we evaluate the methods implemented in this chapter in order to find out which are performing best.
% ------------------------------------------------------------------------


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
